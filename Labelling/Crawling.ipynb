{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRAWLING KOMENTAR BBC INDONESIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>likeCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-02T11:02:48Z</td>\n",
       "      <td>@uunaja2278</td>\n",
       "      <td>Kalou ke bogor awas guah buang ke gunung salak...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-29T08:43:36Z</td>\n",
       "      <td>@fitryyusub107</td>\n",
       "      <td>ðŸ‘ŒðŸŒŸ &lt;a href=\"https://www.youtube.com/watch?v=Ap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-16T03:34:12Z</td>\n",
       "      <td>@mawardiwalker2687</td>\n",
       "      <td>Mgkin bnyak pengedar ganja nanti nya...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-13T14:34:41Z</td>\n",
       "      <td>@busupi5841</td>\n",
       "      <td>Bilang mampu memberi makan orang rohingya?coba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-13T14:28:51Z</td>\n",
       "      <td>@busupi5841</td>\n",
       "      <td>Itu yang bikang mampu  mau memberi makan orang...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9560</th>\n",
       "      <td>2020-06-27T03:51:34Z</td>\n",
       "      <td>@zaczain5654</td>\n",
       "      <td>Niat kita semuanya baik. Termasuk kami di Mala...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9561</th>\n",
       "      <td>2020-06-27T07:42:10Z</td>\n",
       "      <td>@garataphone6149</td>\n",
       "      <td>Amin,,,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9562</th>\n",
       "      <td>2020-06-27T09:45:41Z</td>\n",
       "      <td>@himaxaja8533</td>\n",
       "      <td>Makasih bang,ni kali ke 5 mrk terdampar ke ace...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9563</th>\n",
       "      <td>2020-06-27T12:46:06Z</td>\n",
       "      <td>@mf4547</td>\n",
       "      <td>Di Rohingya sudah kembali normal lagi tidak se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9564</th>\n",
       "      <td>2020-06-26T12:33:20Z</td>\n",
       "      <td>@stevenslcky2849</td>\n",
       "      <td>Respect</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9565 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               publishedAt   authorDisplayName  \\\n",
       "0     2024-05-02T11:02:48Z         @uunaja2278   \n",
       "1     2024-04-29T08:43:36Z      @fitryyusub107   \n",
       "2     2024-04-16T03:34:12Z  @mawardiwalker2687   \n",
       "3     2024-04-13T14:34:41Z         @busupi5841   \n",
       "4     2024-04-13T14:28:51Z         @busupi5841   \n",
       "...                    ...                 ...   \n",
       "9560  2020-06-27T03:51:34Z        @zaczain5654   \n",
       "9561  2020-06-27T07:42:10Z    @garataphone6149   \n",
       "9562  2020-06-27T09:45:41Z       @himaxaja8533   \n",
       "9563  2020-06-27T12:46:06Z             @mf4547   \n",
       "9564  2020-06-26T12:33:20Z    @stevenslcky2849   \n",
       "\n",
       "                                            textDisplay  likeCount  \n",
       "0     Kalou ke bogor awas guah buang ke gunung salak...          0  \n",
       "1     ðŸ‘ŒðŸŒŸ <a href=\"https://www.youtube.com/watch?v=Ap...          1  \n",
       "2               Mgkin bnyak pengedar ganja nanti nya...          0  \n",
       "3     Bilang mampu memberi makan orang rohingya?coba...          0  \n",
       "4     Itu yang bikang mampu  mau memberi makan orang...          0  \n",
       "...                                                 ...        ...  \n",
       "9560  Niat kita semuanya baik. Termasuk kami di Mala...          8  \n",
       "9561                                            Amin,,,          0  \n",
       "9562  Makasih bang,ni kali ke 5 mrk terdampar ke ace...          1  \n",
       "9563  Di Rohingya sudah kembali normal lagi tidak se...          1  \n",
       "9564                                            Respect          2  \n",
       "\n",
       "[9565 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Inisialisasi objek YouTube API\n",
    "from googleapiclient.discovery import build\n",
    "api_key = 'AIzaSyC-5ezQ7iiiGT_6fB8CyyqVASnGvUsKksY'\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Fungsi untuk mendapatkan komentar dan balasan\n",
    "def video_comments(video_id):\n",
    "    comments_list = []\n",
    "    video_response = youtube.commentThreads().list(part='snippet,replies', videoId=video_id).execute()\n",
    "\n",
    "    while video_response:\n",
    "        for item in video_response.get('items', []):\n",
    "            published = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "            user = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            likeCount = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "            comments_list.append([published, user, comment, likeCount])\n",
    "\n",
    "            replycount = item['snippet']['totalReplyCount']\n",
    "            if replycount > 0:\n",
    "                for reply_item in item['replies']['comments']:\n",
    "                    reply_published = reply_item['snippet']['publishedAt']\n",
    "                    reply_user = reply_item['snippet']['authorDisplayName']\n",
    "                    reply_comment = reply_item['snippet']['textDisplay']\n",
    "                    reply_likeCount = reply_item['snippet']['likeCount']\n",
    "                    comments_list.append([reply_published, reply_user, reply_comment, reply_likeCount])\n",
    "\n",
    "        if 'nextPageToken' in video_response:\n",
    "            video_response = youtube.commentThreads().list(\n",
    "                part='snippet,replies',\n",
    "                videoId=video_id,\n",
    "                pageToken=video_response['nextPageToken']\n",
    "            ).execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments_list\n",
    "\n",
    "# Panggil fungsi untuk mendapatkan komentar\n",
    "video_id = \"ApExODTo354\"  # Ganti dengan ID video yang sesuai\n",
    "comments = video_comments(video_id)\n",
    "\n",
    "# Buat DataFrame\n",
    "df_BBC = pd.DataFrame(comments, columns=['publishedAt', 'authorDisplayName', 'textDisplay', 'likeCount'])\n",
    "\n",
    "# Tampilkan DataFrame\n",
    "df_BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BBC.to_csv('BBC INDONESIA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRAWLING YOUTUBE Kompas.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>likeCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-31T08:55:04Z</td>\n",
       "      <td>@kompascom</td>\n",
       "      <td>Artikel terkait:&lt;br&gt;&lt;a href=\"https://regional....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-08T12:25:31Z</td>\n",
       "      <td>@AriKokok</td>\n",
       "      <td>Kenapa mereka selalu terdampar di Aceh. Kenapa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-22T13:28:28Z</td>\n",
       "      <td>@niksiahaan1380</td>\n",
       "      <td>Nanti buat masalah lagi, contohnya di Malaysia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-12T13:53:16Z</td>\n",
       "      <td>@syariphidayatullah9313</td>\n",
       "      <td>Salut buat Aceh janganlah seperti malingsia mu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-09T16:51:56Z</td>\n",
       "      <td>@arintamatantu5272</td>\n",
       "      <td>Allah Ya Rahman ðŸ˜­ðŸ˜­ðŸ˜­</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2021-12-31T10:58:43Z</td>\n",
       "      <td>@lokmanchejusoh531</td>\n",
       "      <td>Jangan risau nanti mereka akan lolos ke malays...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2021-12-31T11:03:02Z</td>\n",
       "      <td>@mbad.6030</td>\n",
       "      <td>@@direct5220 &lt;a href=\"https://www.youtube.com/...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2021-12-31T09:17:56Z</td>\n",
       "      <td>@anakindonesia2412</td>\n",
       "      <td>kenapa Tak di kasi balik di kampung mereka</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2021-12-31T10:57:04Z</td>\n",
       "      <td>@mbad.6030</td>\n",
       "      <td>Cari lewat samrt phone bang.&lt;br&gt;Jangan kalah s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2021-12-31T08:55:04Z</td>\n",
       "      <td>@kompascom</td>\n",
       "      <td>Artikel terkait:&lt;br&gt;&lt;a href=\"https://regional....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             publishedAt        authorDisplayName  \\\n",
       "0   2021-12-31T08:55:04Z               @kompascom   \n",
       "1   2022-03-08T12:25:31Z                @AriKokok   \n",
       "2   2022-02-22T13:28:28Z          @niksiahaan1380   \n",
       "3   2022-01-12T13:53:16Z  @syariphidayatullah9313   \n",
       "4   2022-01-09T16:51:56Z       @arintamatantu5272   \n",
       "..                   ...                      ...   \n",
       "76  2021-12-31T10:58:43Z       @lokmanchejusoh531   \n",
       "77  2021-12-31T11:03:02Z               @mbad.6030   \n",
       "78  2021-12-31T09:17:56Z       @anakindonesia2412   \n",
       "79  2021-12-31T10:57:04Z               @mbad.6030   \n",
       "80  2021-12-31T08:55:04Z               @kompascom   \n",
       "\n",
       "                                          textDisplay  likeCount  \n",
       "0   Artikel terkait:<br><a href=\"https://regional....          0  \n",
       "1   Kenapa mereka selalu terdampar di Aceh. Kenapa...          0  \n",
       "2   Nanti buat masalah lagi, contohnya di Malaysia...          0  \n",
       "3   Salut buat Aceh janganlah seperti malingsia mu...          0  \n",
       "4                                 Allah Ya Rahman ðŸ˜­ðŸ˜­ðŸ˜­          0  \n",
       "..                                                ...        ...  \n",
       "76  Jangan risau nanti mereka akan lolos ke malays...          2  \n",
       "77  @@direct5220 <a href=\"https://www.youtube.com/...          2  \n",
       "78         kenapa Tak di kasi balik di kampung mereka          8  \n",
       "79  Cari lewat samrt phone bang.<br>Jangan kalah s...          1  \n",
       "80  Artikel terkait:<br><a href=\"https://regional....          0  \n",
       "\n",
       "[81 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Inisialisasi objek YouTube API\n",
    "from googleapiclient.discovery import build\n",
    "api_key = 'AIzaSyC-5ezQ7iiiGT_6fB8CyyqVASnGvUsKksY'\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Fungsi untuk mendapatkan komentar dan balasan\n",
    "def video_comments(video_id):\n",
    "    comments_list = []\n",
    "    video_response = youtube.commentThreads().list(part='snippet,replies', videoId=video_id).execute()\n",
    "\n",
    "    while video_response:\n",
    "        for item in video_response.get('items', []):\n",
    "            published = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "            user = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            likeCount = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "            comments_list.append([published, user, comment, likeCount])\n",
    "\n",
    "            replycount = item['snippet']['totalReplyCount']\n",
    "            if replycount > 0:\n",
    "                for reply_item in item['replies']['comments']:\n",
    "                    reply_published = reply_item['snippet']['publishedAt']\n",
    "                    reply_user = reply_item['snippet']['authorDisplayName']\n",
    "                    reply_comment = reply_item['snippet']['textDisplay']\n",
    "                    reply_likeCount = reply_item['snippet']['likeCount']\n",
    "                    comments_list.append([reply_published, reply_user, reply_comment, reply_likeCount])\n",
    "\n",
    "        if 'nextPageToken' in video_response:\n",
    "            video_response = youtube.commentThreads().list(\n",
    "                part='snippet,replies',\n",
    "                videoId=video_id,\n",
    "                pageToken=video_response['nextPageToken']\n",
    "            ).execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments_list\n",
    "\n",
    "# Panggil fungsi untuk mendapatkan komentar\n",
    "video_id = \"CXKghmMdFEE\"  # Ganti dengan ID video yang sesuai\n",
    "comments = video_comments(video_id)\n",
    "\n",
    "# Buat DataFrame\n",
    "df_kompas = pd.DataFrame(comments, columns=['publishedAt', 'authorDisplayName', 'textDisplay', 'likeCount'])\n",
    "\n",
    "# Tampilkan DataFrame\n",
    "df_kompas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kompas.to_csv('Kompas.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRAWLING YOUTUBE TV One News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>likeCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-10T15:56:53Z</td>\n",
       "      <td>@Pemulungilmu2002</td>\n",
       "      <td>Saya prihatin kasihan sekali melihat orang roh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-07T15:31:14Z</td>\n",
       "      <td>@RandySihombing-sv7mr</td>\n",
       "      <td>Kita ini hdup di atas dunia pun semua nya mnmp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-06T14:54:41Z</td>\n",
       "      <td>@insaniahiin9702</td>\n",
       "      <td>Kerumah Mbak aja tampung semua krn sdh menerim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-05-03T07:46:30Z</td>\n",
       "      <td>@fitryyusub107</td>\n",
       "      <td>ðŸŒŸðŸŒŸ &lt;a href=\"https://www.youtube.com/watch?v=3m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-27T19:19:09Z</td>\n",
       "      <td>@putra_mahesa</td>\n",
       "      <td>UNHCR HANYA MENCARI KEUNTUNGAN UNTUK MENJUAL M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10841</th>\n",
       "      <td>2023-12-25T16:34:33Z</td>\n",
       "      <td>@akudan8837</td>\n",
       "      <td>@@pg778 ah masa sih ? Kmu kehabisan obat ya? M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10842</th>\n",
       "      <td>2023-12-25T16:00:01Z</td>\n",
       "      <td>@Ricky_Garong</td>\n",
       "      <td>Kalau kita terima yang lain ikutan datang, bis...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10843</th>\n",
       "      <td>2023-12-25T16:38:02Z</td>\n",
       "      <td>@suhardichenel3616</td>\n",
       "      <td>Betul banget,setuju usir,pulangkan,tolak Rohingya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10844</th>\n",
       "      <td>2023-12-25T15:59:06Z</td>\n",
       "      <td>@bss.chanel8116</td>\n",
       "      <td>Balikan ke negaranya lama kelamaan mereka bisa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10845</th>\n",
       "      <td>2023-12-25T15:58:30Z</td>\n",
       "      <td>@alisulialiofficial</td>\n",
       "      <td>Semangat warga aceh semoga perjuangan kalian t...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10846 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                publishedAt      authorDisplayName  \\\n",
       "0      2024-05-10T15:56:53Z      @Pemulungilmu2002   \n",
       "1      2024-05-07T15:31:14Z  @RandySihombing-sv7mr   \n",
       "2      2024-05-06T14:54:41Z       @insaniahiin9702   \n",
       "3      2024-05-03T07:46:30Z         @fitryyusub107   \n",
       "4      2024-04-27T19:19:09Z          @putra_mahesa   \n",
       "...                     ...                    ...   \n",
       "10841  2023-12-25T16:34:33Z            @akudan8837   \n",
       "10842  2023-12-25T16:00:01Z          @Ricky_Garong   \n",
       "10843  2023-12-25T16:38:02Z     @suhardichenel3616   \n",
       "10844  2023-12-25T15:59:06Z        @bss.chanel8116   \n",
       "10845  2023-12-25T15:58:30Z    @alisulialiofficial   \n",
       "\n",
       "                                             textDisplay  likeCount  \n",
       "0      Saya prihatin kasihan sekali melihat orang roh...          0  \n",
       "1      Kita ini hdup di atas dunia pun semua nya mnmp...          0  \n",
       "2      Kerumah Mbak aja tampung semua krn sdh menerim...          0  \n",
       "3      ðŸŒŸðŸŒŸ <a href=\"https://www.youtube.com/watch?v=3m...          0  \n",
       "4      UNHCR HANYA MENCARI KEUNTUNGAN UNTUK MENJUAL M...          0  \n",
       "...                                                  ...        ...  \n",
       "10841  @@pg778 ah masa sih ? Kmu kehabisan obat ya? M...          0  \n",
       "10842  Kalau kita terima yang lain ikutan datang, bis...          9  \n",
       "10843  Betul banget,setuju usir,pulangkan,tolak Rohingya          0  \n",
       "10844  Balikan ke negaranya lama kelamaan mereka bisa...          0  \n",
       "10845  Semangat warga aceh semoga perjuangan kalian t...         19  \n",
       "\n",
       "[10846 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Inisialisasi objek YouTube API\n",
    "from googleapiclient.discovery import build\n",
    "api_key = 'AIzaSyC-5ezQ7iiiGT_6fB8CyyqVASnGvUsKksY'\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Fungsi untuk mendapatkan komentar dan balasan\n",
    "def video_comments(video_id):\n",
    "    comments_list = []\n",
    "    video_response = youtube.commentThreads().list(part='snippet,replies', videoId=video_id).execute()\n",
    "\n",
    "    while video_response:\n",
    "        for item in video_response.get('items', []):\n",
    "            published = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "            user = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            likeCount = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "            comments_list.append([published, user, comment, likeCount])\n",
    "\n",
    "            replycount = item['snippet']['totalReplyCount']\n",
    "            if replycount > 0:\n",
    "                for reply_item in item['replies']['comments']:\n",
    "                    reply_published = reply_item['snippet']['publishedAt']\n",
    "                    reply_user = reply_item['snippet']['authorDisplayName']\n",
    "                    reply_comment = reply_item['snippet']['textDisplay']\n",
    "                    reply_likeCount = reply_item['snippet']['likeCount']\n",
    "                    comments_list.append([reply_published, reply_user, reply_comment, reply_likeCount])\n",
    "\n",
    "        if 'nextPageToken' in video_response:\n",
    "            video_response = youtube.commentThreads().list(\n",
    "                part='snippet,replies',\n",
    "                videoId=video_id,\n",
    "                pageToken=video_response['nextPageToken']\n",
    "            ).execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments_list\n",
    "\n",
    "# Panggil fungsi untuk mendapatkan komentar\n",
    "video_id = \"3mtGKBVy2zw\"  # Ganti dengan ID video yang sesuai\n",
    "comments = video_comments(video_id)\n",
    "\n",
    "# Buat DataFrame\n",
    "df_tvone = pd.DataFrame(comments, columns=['publishedAt', 'authorDisplayName', 'textDisplay', 'likeCount'])\n",
    "\n",
    "# Tampilkan DataFrame\n",
    "df_tvone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tvone.to_csv('TV One News')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRAWLING YOUTUBE CNN Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>likeCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-14T10:07:45Z</td>\n",
       "      <td>@andregendut4551</td>\n",
       "      <td>Lihat lah begitu JD Kumu kotor EMG begitu bany...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-13T16:44:38Z</td>\n",
       "      <td>@UripTarno</td>\n",
       "      <td>Waspada indonesia  ini sangat berbahaya.pemeri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-13T16:35:42Z</td>\n",
       "      <td>@UripTarno</td>\n",
       "      <td>Kalo ga di usir siap siap aja indonesia hancur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-05-09T11:57:58Z</td>\n",
       "      <td>@sallsabillachanel5319</td>\n",
       "      <td>Ini pemerintah gak ada tindakan apa2 kah? Baha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-09T10:00:15Z</td>\n",
       "      <td>@user-oi1di7id5g</td>\n",
       "      <td>Jgan bnyak d tanya ..nti jwab nya ngawur ....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>2023-12-25T09:36:40Z</td>\n",
       "      <td>@rafkysyaputra6541</td>\n",
       "      <td>ðŸ˜ƒðŸ‘†ðŸ‘†</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>2023-12-25T10:04:56Z</td>\n",
       "      <td>@irwantos.t3510</td>\n",
       "      <td>Tempat terbaik untuk Rohingnya adalah kutub se...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>2023-12-25T10:08:25Z</td>\n",
       "      <td>@rafkysyaputra6541</td>\n",
       "      <td>@@irwantos.t3510 dijadikan budak aja</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>2023-12-25T10:08:27Z</td>\n",
       "      <td>@monicayamamoto</td>\n",
       "      <td>Hahahaa Indonesian Muslim country but refusing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>2023-12-25T10:31:32Z</td>\n",
       "      <td>@mommyfatih056</td>\n",
       "      <td>@@monicayamamoto not refugees. Bcos they alrea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2118 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               publishedAt       authorDisplayName  \\\n",
       "0     2024-05-14T10:07:45Z        @andregendut4551   \n",
       "1     2024-05-13T16:44:38Z              @UripTarno   \n",
       "2     2024-05-13T16:35:42Z              @UripTarno   \n",
       "3     2024-05-09T11:57:58Z  @sallsabillachanel5319   \n",
       "4     2024-05-09T10:00:15Z        @user-oi1di7id5g   \n",
       "...                    ...                     ...   \n",
       "2113  2023-12-25T09:36:40Z      @rafkysyaputra6541   \n",
       "2114  2023-12-25T10:04:56Z         @irwantos.t3510   \n",
       "2115  2023-12-25T10:08:25Z      @rafkysyaputra6541   \n",
       "2116  2023-12-25T10:08:27Z         @monicayamamoto   \n",
       "2117  2023-12-25T10:31:32Z          @mommyfatih056   \n",
       "\n",
       "                                            textDisplay  likeCount  \n",
       "0     Lihat lah begitu JD Kumu kotor EMG begitu bany...          0  \n",
       "1     Waspada indonesia  ini sangat berbahaya.pemeri...          0  \n",
       "2     Kalo ga di usir siap siap aja indonesia hancur...          0  \n",
       "3     Ini pemerintah gak ada tindakan apa2 kah? Baha...          0  \n",
       "4         Jgan bnyak d tanya ..nti jwab nya ngawur ....          0  \n",
       "...                                                 ...        ...  \n",
       "2113                                                ðŸ˜ƒðŸ‘†ðŸ‘†          0  \n",
       "2114  Tempat terbaik untuk Rohingnya adalah kutub se...          2  \n",
       "2115               @@irwantos.t3510 dijadikan budak aja          0  \n",
       "2116  Hahahaa Indonesian Muslim country but refusing...          0  \n",
       "2117  @@monicayamamoto not refugees. Bcos they alrea...          1  \n",
       "\n",
       "[2118 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Inisialisasi objek YouTube API\n",
    "from googleapiclient.discovery import build\n",
    "api_key = 'AIzaSyC-5ezQ7iiiGT_6fB8CyyqVASnGvUsKksY'\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Fungsi untuk mendapatkan komentar dan balasan\n",
    "def video_comments(video_id):\n",
    "    comments_list = []\n",
    "    video_response = youtube.commentThreads().list(part='snippet,replies', videoId=video_id).execute()\n",
    "\n",
    "    while video_response:\n",
    "        for item in video_response.get('items', []):\n",
    "            published = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "            user = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            likeCount = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "            comments_list.append([published, user, comment, likeCount])\n",
    "\n",
    "            replycount = item['snippet']['totalReplyCount']\n",
    "            if replycount > 0:\n",
    "                for reply_item in item['replies']['comments']:\n",
    "                    reply_published = reply_item['snippet']['publishedAt']\n",
    "                    reply_user = reply_item['snippet']['authorDisplayName']\n",
    "                    reply_comment = reply_item['snippet']['textDisplay']\n",
    "                    reply_likeCount = reply_item['snippet']['likeCount']\n",
    "                    comments_list.append([reply_published, reply_user, reply_comment, reply_likeCount])\n",
    "\n",
    "        if 'nextPageToken' in video_response:\n",
    "            video_response = youtube.commentThreads().list(\n",
    "                part='snippet,replies',\n",
    "                videoId=video_id,\n",
    "                pageToken=video_response['nextPageToken']\n",
    "            ).execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments_list\n",
    "\n",
    "# Panggil fungsi untuk mendapatkan komentar\n",
    "video_id = \"63DtGdILd-A\"  # Ganti dengan ID video yang sesuai\n",
    "comments = video_comments(video_id)\n",
    "\n",
    "# Buat DataFrame\n",
    "df_CNN = pd.DataFrame(comments, columns=['publishedAt', 'authorDisplayName', 'textDisplay', 'likeCount'])\n",
    "\n",
    "# Tampilkan DataFrame\n",
    "df_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN.to_csv('CNN Indonesia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRAWLING YOUTUBE Serambinews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>likeCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-09T02:56:51Z</td>\n",
       "      <td>@SamSam_2021</td>\n",
       "      <td>JGN SESEKALI BELA BIAWAK HIDUP...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-19T04:18:40Z</td>\n",
       "      <td>@user-kj8kn1fo4x</td>\n",
       "      <td>Tenggelamkan aja kapal nya selesai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-19T04:18:17Z</td>\n",
       "      <td>@user-kj8kn1fo4x</td>\n",
       "      <td>Itulah manusia iblis yg datang berpurah2 minta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-07T09:43:18Z</td>\n",
       "      <td>@user-cf5ko6zh7n</td>\n",
       "      <td>Rohingya klo kurang ajar kasih keluar saja dar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-06T03:31:27Z</td>\n",
       "      <td>@JauharMustofa-vc1im</td>\n",
       "      <td>Sdh tahu sifat dan tabiatnya pantes di tolak d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>2023-11-23T01:37:15Z</td>\n",
       "      <td>@aeromedia.</td>\n",
       "      <td>Mereka berbeda jauh dari rakyat Palestina.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>2023-11-23T01:41:40Z</td>\n",
       "      <td>@trisyustina2640</td>\n",
       "      <td>Jgn bawa&amp;quot; Islam klo km kasian  kasih aja ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>2023-11-23T01:42:06Z</td>\n",
       "      <td>@Fansboy992</td>\n",
       "      <td>â€‹@@aeromedia.tapi bukankah mereka manusia juga?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5883</th>\n",
       "      <td>2023-11-23T02:09:55Z</td>\n",
       "      <td>@RandiRama-kw6qx</td>\n",
       "      <td>â€‹@@Fansboy992yaudah taro rumah kau aja semisal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>2023-11-23T02:10:27Z</td>\n",
       "      <td>@AyongHendra-zh4nc</td>\n",
       "      <td>Sususuu to so to so</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5885 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               publishedAt     authorDisplayName  \\\n",
       "0     2024-04-09T02:56:51Z          @SamSam_2021   \n",
       "1     2024-03-19T04:18:40Z      @user-kj8kn1fo4x   \n",
       "2     2024-03-19T04:18:17Z      @user-kj8kn1fo4x   \n",
       "3     2024-03-07T09:43:18Z      @user-cf5ko6zh7n   \n",
       "4     2024-03-06T03:31:27Z  @JauharMustofa-vc1im   \n",
       "...                    ...                   ...   \n",
       "5880  2023-11-23T01:37:15Z           @aeromedia.   \n",
       "5881  2023-11-23T01:41:40Z      @trisyustina2640   \n",
       "5882  2023-11-23T01:42:06Z           @Fansboy992   \n",
       "5883  2023-11-23T02:09:55Z      @RandiRama-kw6qx   \n",
       "5884  2023-11-23T02:10:27Z    @AyongHendra-zh4nc   \n",
       "\n",
       "                                            textDisplay  likeCount  \n",
       "0                     JGN SESEKALI BELA BIAWAK HIDUP...          1  \n",
       "1                    Tenggelamkan aja kapal nya selesai          0  \n",
       "2     Itulah manusia iblis yg datang berpurah2 minta...          0  \n",
       "3     Rohingya klo kurang ajar kasih keluar saja dar...          0  \n",
       "4     Sdh tahu sifat dan tabiatnya pantes di tolak d...          0  \n",
       "...                                                 ...        ...  \n",
       "5880         Mereka berbeda jauh dari rakyat Palestina.          2  \n",
       "5881  Jgn bawa&quot; Islam klo km kasian  kasih aja ...          2  \n",
       "5882    â€‹@@aeromedia.tapi bukankah mereka manusia juga?          1  \n",
       "5883  â€‹@@Fansboy992yaudah taro rumah kau aja semisal...          0  \n",
       "5884                                Sususuu to so to so          0  \n",
       "\n",
       "[5885 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Inisialisasi objek YouTube API\n",
    "from googleapiclient.discovery import build\n",
    "api_key = 'AIzaSyC-5ezQ7iiiGT_6fB8CyyqVASnGvUsKksY'\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Fungsi untuk mendapatkan komentar dan balasan\n",
    "def video_comments(video_id):\n",
    "    comments_list = []\n",
    "    video_response = youtube.commentThreads().list(part='snippet,replies', videoId=video_id).execute()\n",
    "\n",
    "    while video_response:\n",
    "        for item in video_response.get('items', []):\n",
    "            published = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "            user = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            likeCount = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "            comments_list.append([published, user, comment, likeCount])\n",
    "\n",
    "            replycount = item['snippet']['totalReplyCount']\n",
    "            if replycount > 0:\n",
    "                for reply_item in item['replies']['comments']:\n",
    "                    reply_published = reply_item['snippet']['publishedAt']\n",
    "                    reply_user = reply_item['snippet']['authorDisplayName']\n",
    "                    reply_comment = reply_item['snippet']['textDisplay']\n",
    "                    reply_likeCount = reply_item['snippet']['likeCount']\n",
    "                    comments_list.append([reply_published, reply_user, reply_comment, reply_likeCount])\n",
    "\n",
    "        if 'nextPageToken' in video_response:\n",
    "            video_response = youtube.commentThreads().list(\n",
    "                part='snippet,replies',\n",
    "                videoId=video_id,\n",
    "                pageToken=video_response['nextPageToken']\n",
    "            ).execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments_list\n",
    "\n",
    "# Panggil fungsi untuk mendapatkan komentar\n",
    "video_id = \"QMGRBcvSri4\"  # Ganti dengan ID video yang sesuai\n",
    "comments = video_comments(video_id)\n",
    "\n",
    "# Buat DataFrame\n",
    "df_Serambi = pd.DataFrame(comments, columns=['publishedAt', 'authorDisplayName', 'textDisplay', 'likeCount'])\n",
    "\n",
    "# Tampilkan DataFrame\n",
    "df_Serambi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Serambi.to_csv('Serambinews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCREPING BERITA/ARTKEL BBC INDONESIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_detik(hal):\n",
    "    global hades\n",
    "    a = 1\n",
    "    for page in range(1,hal):\n",
    "        url = f'https://www.detik.com/search/searchall?query=rohingya+di+aceh&sortby=time&page={page}'\n",
    "        ge = req.get(url,hades).text\n",
    "        sop = bs(ge, 'lxml')\n",
    "        li = sop.find('div',class_='list media_rows list_berita')\n",
    "        lin = li.find_all('articel')\n",
    "        for x in lin:\n",
    "            link = x.find('a')['href']\n",
    "            date = x.find('a').find('span',class_='date').text.replace('WIB','').replace('detikNews','').split(',')[1]\n",
    "            headline = x.find('a').find('h2').text\n",
    "            ge_ = req.get(link,hades).text\n",
    "            sop_= bs(ge_,'lxml')\n",
    "            content = sop_.find_all('div',class_='detail__body-text itp_bodycontent')\n",
    "            for x in content:\n",
    "                x = x.find_all('p')\n",
    "                y = [y.text for y in x ]\n",
    "                content_ = ''.join(y).replace('\\n', '').replace('ADVERTISEMENT','').replace('SCROLL TO RESUME CONTENT','')\n",
    "                print(f'done[{a}] >{headline[0:10]}')\n",
    "                a += 1\n",
    "                with open('Berita.csv', 'a')as file:\n",
    "                    wr = csv.writer(file, delimeter=',')\n",
    "                    wr.writerow([headline,date,link,content_])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_detik(pages):\n",
    "    base_url = \"https://www.detik.com/search/searchall?query=rohingya+di+aceh&sortby=time&result_type=latest&page=56\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'}\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f\"{base_url}/{page}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            news_items = soup.find_all('article')\n",
    "\n",
    "            for item in news_items:\n",
    "                try:\n",
    "                    title = item.find('h3').get_text().strip()\n",
    "                    link = item.find('a')['href']\n",
    "                    \n",
    "                    # Scrape the article content\n",
    "                    article_response = requests.get(link, headers=headers)\n",
    "                    article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "                    article_content = article_soup.find('div', class_='detail__body-text itp_bodycontent').get_text(separator='\\n').strip()\n",
    "                    \n",
    "                    articles.append([title, link, article_content])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error scraping article: {e}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve page {page}\")\n",
    "\n",
    "    # Save the articles to a CSV file\n",
    "    with open('detik_articles.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Title', 'Link', 'Content'])\n",
    "        writer.writerows(articles)\n",
    "    \n",
    "    print(f\"Scraped {len(articles)} articles and saved to detik_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_detik(56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_detik(pages):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    article_count = 1\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f'https://www.detik.com/search/searchall?query=rohingya+di+aceh&sortby=time&page={page}'\n",
    "        response = req.get(url, headers=headers)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "\n",
    "        article_list = soup.find('div', class_='list media_rows list_berita')\n",
    "        if not article_list:\n",
    "            continue\n",
    "\n",
    "        articles = article_list.find_all('article')\n",
    "        for article in articles:\n",
    "            try:\n",
    "                link = article.find('a')['href']\n",
    "                date = article.find('span', class_='date').text.replace('WIB', '').replace('BBCIndonesia', '').split(',')[1].strip()\n",
    "                headline = article.find('h2').text.strip()\n",
    "\n",
    "                article_response = req.get(link, headers=headers)\n",
    "                article_soup = bs(article_response.text, 'lxml')\n",
    "                content_div = article_soup.find('div', class_='detail__body-text itp_bodycontent')\n",
    "\n",
    "                if not content_div:\n",
    "                    continue\n",
    "\n",
    "                paragraphs = content_div.find_all('p')\n",
    "                content = ' '.join([p.get_text() for p in paragraphs])\n",
    "                content = content.replace('\\n', '').replace('ADVERTISEMENT', '').replace('SCROLL TO RESUME CONTENT', '')\n",
    "\n",
    "                print(f'done[{article_count}] > {headline[0:10]}')\n",
    "                article_count += 1\n",
    "\n",
    "                with open('Berita_Digital.csv', 'a', newline='', encoding='utf-8') as file:\n",
    "                    writer = csv.writer(file, delimiter=',')\n",
    "                    writer.writerow([headline, date, link, content])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article: {e}\")\n",
    "\n",
    "            \n",
    "            df_berita_digital = pd.DataFrame(comments, columns=['link', 'content', 'text'])\n",
    "\n",
    "            \n",
    "            df_berita_digital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_detik(56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_berita_digital' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_berita_digital\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBerita_Digital\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_berita_digital' is not defined"
     ]
    }
   ],
   "source": [
    "df_berita_digital.to_csv('Berita_Digital')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
